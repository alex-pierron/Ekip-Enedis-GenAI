{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\alex\\anaconda3\\envs\\ekip_env_enedis\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader('pdf\\Revue-Médias - DR Nord-Pas-de-Calais du 19122024.pdf')\n",
    "page = reader.pages[5]\n",
    "extracted_text = page.extract_text()\n",
    "print(extracted_text =='DR Nord-Pas-de-Calais')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'articles extraits : 42\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_articles_from_pdf(pdf_path, dossier=False):\n",
    "    # Initialisation du lecteur de PDF\n",
    "    reader = PdfReader(pdf_path)\n",
    "    all_text = ''\n",
    "    \n",
    "    # Liste pour stocker les pages qui seront incluses dans les articles\n",
    "    pages_text = []\n",
    "    \n",
    "    # Si dossier=True, ignorer les pages avant la première page contenant 'DR Nord-Pas-de-Calais' uniquement\n",
    "    if dossier:\n",
    "        found_first_article = False\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            \n",
    "            # Si la page contient seulement 'DR Nord-Pas-de-Calais', l'ignorer\n",
    "            if page_text.strip() == 'DR Nord-Pas-de-Calais':\n",
    "                found_first_article = True\n",
    "                continue  # Passer à la page suivante sans ajouter cette page\n",
    "            \n",
    "            elif found_first_article:  # Ajouter les pages pertinentes après la première page d'article\n",
    "                pages_text.append(page_text)\n",
    "    else:\n",
    "        # Si dossier=False, on ajoute simplement toutes les pages\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            pages_text.append(page_text)\n",
    "    \n",
    "    # Joindre toutes les pages extraites en un seul texte\n",
    "    all_text = \"\\n\".join(pages_text)\n",
    "    \n",
    "    # Expression régulière pour identifier les articles en fonction de la balise \"Parution\"\n",
    "    # Nous recherchons \"Parution\" pour délimiter les fins d'article\n",
    "    articles = []\n",
    "    current_article = []\n",
    "    \n",
    "    # Séparer le texte sur chaque occurrence de la balise \"Parution\"\n",
    "    for page_num, page_text in enumerate(pages_text):\n",
    "        # Si la page contient la balise \"Parution\"\n",
    "        if \"Parution\" in page_text:\n",
    "            # Ajouter la page courante contenant la balise \"Parution\"\n",
    "            current_article.append(page_text)\n",
    "            \n",
    "            # Ajouter l'article complet à la liste des articles\n",
    "            articles.append(\"\\n\".join(current_article))\n",
    "            \n",
    "            # Réinitialiser pour commencer un nouvel article\n",
    "            current_article = []\n",
    "        else:\n",
    "            # Ajouter la page à l'article en cours\n",
    "            current_article.append(page_text)\n",
    "    \n",
    "    # Si un article est encore en cours après la dernière page, l'ajouter aussi\n",
    "    if current_article:\n",
    "        articles.append(\"\\n\".join(current_article))\n",
    "    \n",
    "    # Nettoyer les articles en enlevant les espaces superflus\n",
    "    articles = [article.strip() for article in articles if article.strip()]\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Exemple d'utilisation\n",
    "pdf_path = 'pdf\\Revue-Médias - DR Nord-Pas-de-Calais du 12122024.pdf'\n",
    "\n",
    "# Si dossier=True, les pages avant la page contenant 'DR Nord-Pas-de-Calais' sont exclues\n",
    "articles = extract_articles_from_pdf(pdf_path, dossier=True)\n",
    "\n",
    "# Affichage du nombre d'articles extraits\n",
    "print(f'Nombre d\\'articles extraits : {len(articles)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGARDER (00:00:44) \n",
      "Émission du 7 décembre 2024 de 17:00 à 17:30\n",
      "[Reportage] Tempête Darragh : 1100 coupures d'électricité\n",
      "enregistrées dans le nord à 16h\n",
      "    \n",
      "Mot(s) clé(s) : \n",
      "Reportage, Enedis DR Nord Pas de Calais, Tempête Darragh, 1100 coupures d'électricité \n",
      "17:01:58  -  [Reportage]  La  tempête  Darragh  a  soufflé  fort  dans  le  Nord-Pas-de-Calais  :  1100\n",
      "coupures d'électricité étaient enregistrées par Enedis à 16h. Des arbres sont tombés sur les lignes à\n",
      "cause du vent. Une cinquantaine d'agents sont mobilisés. \n",
      "17:02:37  - Fin. \n",
      "↑ 9\n",
      "SUR LE VIF\n",
      "Quel hélico ?\n",
      " jeudi 12 au jeudi 19 décembre 2024\n",
      "Édition(s) : Edition Principale\n",
      "Page 2\n",
      "34 mots - < 1 min\n",
      "  \n",
      "   \n",
      "Dans les jours à venir, vous\n",
      "pourrez  apercevoir  un\n",
      "hélicoptère survoler le secteur.C’est Enedis qui surveillera les\n",
      "installations électriques, possi‐\n",
      "blement jusque fin janvier.  ■\n",
      "T ous droits réservés L'Echo de la Lys 2024\n",
      "5967e9a08dc0520d709930f6a305318a080AadB44Qa9X9eb28b1\n",
      "613\n",
      " Parution : Hebdomadaire\n",
      "Diffusion : 4684 ex. (Diff. payée Fr.) - © ACPM DSH 2020-2021↑ 10\n"
     ]
    }
   ],
   "source": [
    "# Affichage du premier article (à titre d'exemple)\n",
    "print(articles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction de la date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date extraite : 12/05/2024\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Fonction pour extraire la date en pleine lettre dans le texte\n",
    "def extract_date_from_text(text):\n",
    "    # Expression régulière pour capturer la date en pleine lettre (jour, mois, année)\n",
    "    date_pattern = r'\\b(?:lundi|mardi|mercredi|jeudi|vendredi|samedi|dimanche)\\s+(\\d{1,2})\\s+([a-zA-Zéàôù]+)\\s+(\\d{4})\\b'\n",
    "    \n",
    "    # Recherche de la date avec l'expression régulière\n",
    "    match = re.search(date_pattern, text)\n",
    "    if match:\n",
    "        # Extraire le jour, le mois et l'année\n",
    "        day, month, year = match.groups()\n",
    "        \n",
    "        # Convertir le mois en texte à son format numérique\n",
    "        months = {\n",
    "            'janvier': '01', 'février': '02', 'mars': '03', 'avril': '04',\n",
    "            'mai': '05', 'juin': '06', 'juillet': '07', 'août': '08',\n",
    "            'septembre': '09', 'octobre': '10', 'novembre': '11', 'décembre': '12'\n",
    "        }\n",
    "        \n",
    "        # Récupérer le mois au format numérique\n",
    "        month_num = months.get(month.lower())\n",
    "        \n",
    "        if month_num:\n",
    "            # Construire la date au format mois/jour/année\n",
    "            date_str = f\"{month_num}/{int(day):02d}/{year}\"\n",
    "            \n",
    "            # Retourner la date formatée\n",
    "            return date_str\n",
    "    \n",
    "    # Si aucune date n'est trouvée\n",
    "    return None\n",
    "\n",
    "# Test de la fonction\n",
    "article = articles[-1]\n",
    "\n",
    "# Extraction de la date\n",
    "date_extracted = extract_date_from_text(article)\n",
    "print(f\"Date extraite : {date_extracted}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction du nom du journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"L'Écho de la Lys\"]\n",
      "[\"L'Écho de la Lys\"]\n",
      "['La Voix du Nord']\n",
      "['La Voix du Nord']\n",
      "['Nord Éclair']\n",
      "['Nord Éclair']\n",
      "['La Voix du Nord']\n",
      "['Nord Éclair']\n",
      "[\"L'Observateur\"]\n",
      "[\"L'Abeille de la Ternoise\"]\n",
      "['Nord Éclair']\n",
      "['La Voix du Nord']\n",
      "['La Voix du Nord']\n",
      "['La Semaine dans le Boulonnais']\n",
      "['Le Journal des Flandres']\n",
      "['La Voix du Nord']\n",
      "['La Voix du Nord']\n",
      "['Nord Éclair']\n",
      "['Nord Littoral']\n",
      "['Nord Littoral']\n",
      "['Nord Littoral']\n",
      "['Nord Littoral']\n",
      "['La Voix du Nord']\n",
      "['La Voix du Nord']\n",
      "['Nord Littoral']\n",
      "['La Voix du Nord']\n",
      "['La Voix du Nord']\n",
      "['Nord Littoral']\n",
      "['La Voix du Nord']\n",
      "['Nord Éclair']\n",
      "['Nord Littoral']\n",
      "['La Voix du Nord']\n",
      "['Nord Littoral']\n",
      "['La Voix du Nord']\n",
      "['Nord Littoral']\n",
      "['La Voix du Nord']\n",
      "['Nord Éclair']\n",
      "['La Voix du Nord']\n",
      "[\"L'Observateur\"]\n",
      "[\"L'Observateur\"]\n",
      "['La Voix du Nord']\n",
      "['La Voix du Nord']\n"
     ]
    }
   ],
   "source": [
    "from media_ressources import media_dict\n",
    "\n",
    "def check_media_in_text(text):\n",
    "    # Normalisation du texte pour une comparaison insensible à la casse\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Liste des variantes de \"Tous droits réservés\" à détecter\n",
    "    reserved_rights_variations = [\n",
    "        \"tous droits réservés\",\n",
    "        \"t ous droits réservés\",  # Possible typo with extra space\n",
    "        \"tous droits reserve\",    # Sans accent\n",
    "        \"tous droits réservée\",    # Petite erreur grammaticale possible\n",
    "        \"tous droit réservé\",      # Singulier\n",
    "        \"tous droit réservés\",     # Mélange singulier/pluriel\n",
    "        \"© tous droits réservés\",  # Avec le symbole copyright\n",
    "        \"(c) tous droits réservés\",\n",
    "        \"tous droits réservés.\",   # Avec un point final\n",
    "        \"tous droits réservés :\",\n",
    "    ]\n",
    "\n",
    "    # Trouver la première occurrence de \"Tous droits réservés\" parmi ses variantes\n",
    "    reserved_rights_index = -1\n",
    "    for variant in reserved_rights_variations:\n",
    "        index = text_lower.find(variant)\n",
    "        if index != -1 and (reserved_rights_index == -1 or index < reserved_rights_index):\n",
    "            reserved_rights_index = index\n",
    "\n",
    "    # Liste des médias trouvés\n",
    "    found_media = []\n",
    "    \n",
    "    # Vérification de chaque média dans le dictionnaire\n",
    "    for media, variations in media_dict.items():\n",
    "        for variation in variations:\n",
    "            # Vérification d'une correspondance exacte de chaque variation dans le texte\n",
    "            variation_lower = variation.lower()\n",
    "            index = text_lower.find(variation_lower)\n",
    "\n",
    "            if index != -1:  # Si la variation est trouvée dans le texte\n",
    "                if reserved_rights_index != -1 and index > reserved_rights_index:\n",
    "                    # Si \"Tous droits réservés\" précède ce média, on retourne uniquement celui-ci\n",
    "                    return [media]\n",
    "                found_media.append(media)\n",
    "                break  # Une fois le média trouvé, on passe au suivant\n",
    "\n",
    "    return found_media  # Retourne la liste complète si \"Tous droits réservés\" n'est pas trouvé\n",
    "\n",
    "\n",
    "\n",
    "for article in articles:\n",
    "    resultats = check_media_in_text(article)\n",
    "    print(resultats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAREST // SUR UN CHANTIER RUE DE CAMBLAIN\n",
      "Un ouvrier électrocuté \n",
      "Mercredi 4 décembre vers 15 heures 45, les sapeurspompiers ont porté secours à\n",
      "deux ouvriers victimes d'une électrocution sur un chantier rue de Camblain.\n",
      " N° 2450\n",
      "jeudi 12 au jeudi 19 décembre 2024\n",
      "Page 4\n",
      "180 mots - 1 min\n",
      "  \n",
      "   \n",
      "L'ouvrier forait un trou quand il\n",
      "aurait touché une ligne haute ten‐\n",
      "sion.\n",
      " \n",
      "Aux  manettes  d'un  engin  de\n",
      "chantier de forage, un homme\n",
      "aurait  touché  un  câble  haute\n",
      "tension souterrain. À l'arrivée\n",
      "des  sapeurs-  pompiers  dePernes, Auchel et l'infirmier sa‐\n",
      "peur-pompier  de  Lillers,  re‐\n",
      "joints par une équipe médicale\n",
      "du Smur de Béthune, l'ouvrier\n",
      "(35 ans) était en arrêt cardio‐\n",
      "respiratoire.  Les  secours  sont\n",
      "parvenus à le réanimer avant\n",
      "son  transport  médicalisé  au\n",
      "centre hospitalier d'Arras. Son\n",
      "pronostic vital était engagé.\n",
      "Son collègue (32 ans) a été di‐\n",
      "rigé vers la polyclinique de laClarence à Divion légèrement\n",
      "blessé.\n",
      "Une enquête de gendarmerie a\n",
      "été  ouverte  pour  déterminer\n",
      "les  causes  de  l'accident.  Les\n",
      "services d'Énedis sont interve‐\n",
      "nus  dès  le  lendemain,  jeudi,\n",
      "pour  procéder  aux  répara‐\n",
      "tions.  ■\n",
      "par  Q.d.\n",
      "T ous droits réservés 2024 L'Abeille De La T ernoise\n",
      "8f68b91f8cb0090590113a86a40f5164000Af7B3cQ4eXcf8b2655e\n",
      "e\n",
      " Parution : Hebdomadaire\n",
      "Diffusion : 8425 ex. (Diff. payée Fr.) - © ACPM DSH 2020-2021↑ 24\n"
     ]
    }
   ],
   "source": [
    "article = articles[9]\n",
    "check_media_in_text(article)\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ekip_env_enedis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
